\chapter{Clustering}
\label {chap:cluster}

This chapter is dedicated to clustering tasks, which is used in querying recommendations (section \ref{subsec:approach_formal_query}). It contains basic notions on clustering, the KNN algorithm and a discussion on the issues of it. Section \ref{sec:cluster_what} presents general aspects of clustering, such as types of clustering methods. Section \ref{sec:cluster_knn} presents the K Nearest Neighbours algorithm, what it does and how it does it. After a discussion about possible issues with the aforementioned algorithm (section \ref{subsec:cluster_knn_curse}), some solutions in the form of Kd-trees (section \ref{subsec:cluster_knn_kdtree}) and ball trees (section \ref{subsec:cluster_knn_balltree} this) are presented. Finally, section \ref{sec:cluster_applications} presents some applications to clustering.

\section{What is clustering?}
\label{sec:cluster_what}

Clustering is one of the many fundamental tasks in data mining which falls in the unsupervised learning category (section \ref{sec:ml_types}). The goal of clustering is finding sets of categories within a dataset \cite{clustering}, which as a group are very similar to one another. By grouping similar items together, they can be classified and by extension, new and similar items are classified similarly to their neighbours.

Clustering algorithms have different notions on what a cluster is. Due to this, many methods have been devised, of which the main ones are hierarchical methods and partitioning methods. However, the categorisation is still a subject of discussion, with Han and Kambar (2001) suggesting the main categories of clustering should be \textit{density-based methods, model-based clustering and grid-based methods}.

Hierarchical methods can be done in a top-down or bottom-up fashion and these methods can be further divided into \cite{clustering}:

\begin{itemize}
\item{Agglomerative hierarchical clustering: clusters are created by merging;}
\item{Divisive hierarchical clustering: clusters are created by dividing.}
\end{itemize} 

\section{K-nearest neighbours}
\label{sec:cluster_knn}

The \textbf{K} \textbf{N}earest \textbf{N}eighbours (KNN) algorithm separates a dataset of points into $n$ clusters by looking at the closest $k$ categorised objects \cite{knn}. Suppose there is an item $x$ which needs to be categorised and in its close vicinity there are a certain number of items which are already part of a category. Then the most probable outcome is that $x$ is also a part of that category.

The algorithm operates in 4 steps \cite{knn}:

\begin{enumerate}
\item{A $k$ (positive integer) is specified, along with a new entry}
\item{Select $k$ closest categorised entries}
\item{Determine the most commonly found classification among these entries}
\item{Assign that class to the new entry}
\end{enumerate}

\subsection{Curse of dimensionality}
\label{subsec:cluster_knn_curse}

A main problem of the \textbf{KNN} algorithm is selecting those $k$ closest neighbours. A naive method, which implements an iterative search has a complexity of \textbf{O(N)} on insertion operations. 

An alternative implementation is selecting all neighbours from a $\delta$ distance from the new entry, which can then result in space partitioning based on their coordinates. So, when searching new elements, one has to look only at its neighbouring partitions. However, while that may sound efficient, the number of neighbours is $3^d$, where $d$ is the dimension of the hyperspace. While this seems like a comparison between an item-based complexity versus a dimension-based complexity of the problem, there is an additional aspect to consider. 

Suppose $d=2$, which means inserting a new item requires searching in a square of size $(3\delta)^2$ with 9 paritions for elements which are in a circle of radius $\delta$. The chance of finding the required elements on average is 
\[ \frac{\pi \delta^2}{9 \delta ^ 2} = \frac{\pi}{9} \]

For $d=3$, the chance becomes
\[ \frac{\frac{4}{3} \pi \delta^2}{27 \delta^3} = \frac{4 \pi}{81 \delta} \]

As the number of dimension increase, the chance, which is the volume of the hypersphere with a radius of $\delta$ over the volume of the hypercube of size $3\delta$, converges to zero. Which means as the number of dimensions increases, the chance of finding relevant neighbours decreases. This is called the curse of dimensionality \cite{curse}.

\subsection{K-d tree}
\label{subsec:cluster_knn_kdtree}

A K-d or K-dimensional binary search tree is a dataset of points in a K-dimensional hyperspace which offers a structuring of data in such a way that is easier for querying proximity points \cite{kdtree}. 

Because it is a binary tree, each node has two children, which are determined by a hyperplane cut. At each node, if it doesn't contain a point, it contains a coordinate and a dimension on which the hyperspace is cut. The left and right children refer to the space partitions created by the cut.

When creating a k-d tree, the cuts are performed on the dimension which is the most divisive (it makes the most clear partitioning of the hyperspace). Figure \ref{fig:kdtree} illustrates how the cuts on a 2-dimensional space looks like.

\begin{figure}[b!]
\centering
\includegraphics[width=0.4\textwidth]{kdtree}
\caption{A 2-dimensional plane being cut for a k-d tree. Each line represents a node, with the left/upper or upper/bottom partitions being children of it. The squares with points in them are leafs.}
\label{fig:kdtree}
\end{figure}

\subsection{Ball tree}
\label{subsec:cluster_knn_balltree}

Similar to the k-d tree, ball trees are binary search trees which partition the dataset in a hierarchy, with the leafs containing the interesting points \cite{balltree}. What is different from k-d trees, however, is that each node is a sphere which covers either a point or contains another smaller sphere. They have been proven to be one of the fastest structures for the NN search \cite{balltree}.

\begin{figure}[b!]
\centering
\includegraphics[width=0.4\textwidth]{balltree}
\caption{A 2-dimensional ball tree}
\label{fig:balltree}
\end{figure}

Ball trees are so similar, that in fact, the k-d creation splitting algorithm can be applied on creating one. The steps for this operation are:

\begin{enumerate}
\item{Find the dimension of greater spread;}
\item{Based on the dimension found, determine the central point and set it as pivot for the current node;}
\item{Construct the ball trees which contain the points left and right of the pivot;}
\item{If any new ball contains a single point, the ball is a single point in space.}
\end{enumerate}

For nearest neighbours searching, a depth-first search is performed using a priority queue. During this, there are three cases that may occur (in order):

\begin{enumerate}
\item{If searched item is further than the current ball than it is from all items in the queue, dismiss the current ball;}
\item{If the current ball contains points, update the queue with all of them;}
\item{Search both children, with the closest ball to the desired point first.}
\end{enumerate}

\section{Applications}
\label{sec:cluster_applications}

Clustering is used in malware detection \cite{malware}, where a certain combination of behaviours or function calls determine if the application means well or attempts to harm systems. Another obvious usage of clustering is recommender systems, where similar content is suggested to users to increase time spent on a platform.
