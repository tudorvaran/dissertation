\chapter{Results and evaluation}
\label{chap:results}

This chapter is dedicated to the evaluation of the approach. Section \ref{sec:results_eval} presents the evaluation methodology and some how some precision functions work. The next sections will focus on trying multiple models or adding scalers to the input data. The first two sections, \ref{sec:results_obj} and \ref{sec:results_word2vec}, detail what happens the object detection model and word2vec model, respectively, are changed. Section \ref{sec:results_dimensions} dives into the hyperplane and shows how dimensions influence the precision. The next focus of experiments will be on using scalers, which is detailed in section \ref{sec:results_scaler}. Finally, a short discussion about the results obtained is in section \ref{sec:results_discussion}.

\section{Evaluation}
\label{sec:results_eval}

Because this is an unsupervised approach, there is no definitive truth to compare it to. However, there are photograph datasets online which are pre-labelled, most notably the \textbf{C}ommon \textbf{O}bjects in \textbf{Co}ntext (COCO) datasets \cite{coco}. They contain over 200.000 labelled images with about 100 object categories. For the following experiments, the 2017 validation images dataset has been chosen, with exactly 5.000 pre-labelled pictures due to time constraints (object detection takes 2 seconds on average per image).

By having some notion of ground truth, a comparison between this model and another can be made. The system relies on recommending pictures with similar objects and content and the dataset is labelled with these. One solution would be to define another $map'$ function, but, this time, based on the annotations. Let $map' : X \mapsto \mathbf{N}^{card(categories)} $ be the definition of the function, where $card(categories)$ represents the number of categories found in the labelled dataset. In this case, the function is simply: 
$$ map'(x') = (y'_1, y'_2, ..., y'_{card(categories)}), y'_i = 
	\begin{cases}
		1 & \textrm{the i-th object in categories is in } x' \\
		0 & \textrm{otherwise}
	\end{cases}
$$

Now let's assume that the inverse function $map'^{-1}$ exists, which would allow recommendations to work:
$$ rec'(x', K) = map'^{-1}(nn'(map'(x'), K)) $$
then $rec'$ can be used to evaluate $rec$ (section \ref{subsec:approach_formal_cluster} to some degree. But, it order to do that, some precision functions specific for information retrieval must be mentioned first (sections \ref{subsec:results_eval_map} and \ref{subsec:results_eval_patk}.

\subsection{Mean average precision}
\label{subsec:results_eval_map}

Mean average precision is an evaluation function used for measuring the precision of an information retrieval model. It relies on the fact that a model outputs a number of values, which can be compared only directly by value if they are identical (words for example). In this case, the output is a list of images from the dataset $X$. 

Let $r = \{r_1, r_2, ..., r_{k_r}\} $ be a set of ranked recommendations given by $rec$ for an $x$ and $k_r$ and $r' = \{r'_1, r'_2, ... r_{k_{r'}}\} $ be the set of correct recommendations resulted from $rec'(x, k_{r'})$. Also, let 
$$ r_{both} = \{ r_i | \forall r_i \in r, \exists r_j \in r' s.t. r'_j = r_i \} $$ 
be the set of all $r_i$ values which are found in $r'$, $pos(r_{both_l})$ be the function which returns the position of element $r_{both_l}$ in $r$ and finally, $first(r_{both_l})$ be a function which returns the cardinal of the set created by selecting only elements from $r$ which precede $r_{pos(r_{both_l})}$. Then:
$$ MAP(r, r') = \frac{\sum_{l \in r_{both}}{\frac{first(r_{both_l})}{pos(r_{both_l})}}}{card(r_{both})} $$

\subsection{Precision at k}
\label{subsec:results_eval_patk}

Unlike mean average precision (\ref{subsec:results_eval_map}), \textbf{P}recision \textbf{at} \textbf{K} (P@K) is more versatile due to the possibility of choosing the k value. But it does not offer a precision based on the quality of a rank for a recommendation, it just treats each recommendation the same.

$$P@K(k) = \frac{\textrm{number of items found in first k which are also in } r'}{k} $$

\section{Object detection models}
\label{sec:results_obj}

Training an object detection model on consumer hardware is time-consuming and might take time to achieve acceptable results. While this is entirely possible to do, there are also pre-trained models, with their associated papers, available online. Since the COCO dataset is used, the best fit for it is a model specifically trained on it.

The ImageAI library, which has also been used in the application, makes available a model which relies on residual networks for object detection \cite{image-ai}. On the chosen dataset (COCO 2017 validation 5K images), it is able to identify at least one object in 4850 images ($97\%$ of the whole dataset).

\section{Word2vec model}
\label{sec:results_word2vec}

As mentioned in section \ref{sec:results_obj}, training a model manually would be taking too long to reach an acceptable precision on a traditional hardware. In the same, there is a certain preference for using pre-trained models instead of manually training one. There are a few word2vec models compatible with the gensim python library which can be found on the \textbf{N}ordic \textbf{L}anguage \textbf{P}rocessing \textbf{L}aboratory (NLPL) website \cite{word2vec-datasets}. Those datasets have been trained on english wikipedia, english gigaword fifth edition among others.

The best dataset there is the one trained on Google News texts, which contains a corpus of 100 billion tokens. For this project, it is the only usable one since the others have missing lemmas which are easily usable for keywords, e.g. simple words such as "\textit{home}" are not found.

\section{Dimensions}
\label{sec:results_dimensions}

This section focuses on the set of keywords and how it impacts the final results. Recall from sections \ref{subsec:results_eval_map} and \ref{subsec:results_eval_patk} how \textbf{m}ean \textbf{a}verage \textbf{p}recision (MAP) and P@k work. 

To use these measurements, all input photographs have been queried for $k_{r'}= k_r =15$ entries. The keywords chosen are subsets of the very general themes \{home, city, water, fun, food, sport, nature, animal\}. Because the number of subsets is large and would take days to test, the selection was performed in Monte Carlo simulation fashion: for each subset of $d < 8$ words, 8 random subsets were chosen as representatives.

This experiment has shown that increasing the number of dimensions for the final hyperplane improves the quality of recommendations, as witnessed in figures \ref{fig:dims} and \ref{fig:dims_map}. As reference, selecting 15 random entries from the dataset as recommendations will result in a P@1 of $0.003$.

\begin{figure}[b!]
\centering
\includegraphics[width=0.49\textwidth]{dim_graphs_Pat1}
\includegraphics[width=0.49\textwidth]{dim_graphs_Pat3}
\includegraphics[width=0.49\textwidth]{dim_graphs_Pat5}
\includegraphics[width=0.49\textwidth]{dim_graphs_Pat10}
\caption{P@1, P@3, P@5, P@10 on different dimensions in the hyperplane}
\label{fig:dims}
\end{figure}

\begin{figure}[b!]
\centering
\includegraphics[width=0.8\textwidth]{dim_graphs_map}
\caption{ Mean average precision on different dimensions in the hyperplane}
\label{fig:dims_map}
\end{figure}


\section{Scaler}
\label{sec:results_scaler}

The final points of the hyperplane are randomly dispersed. Using scalers, the hyperplane can be further mapped into another which makes those points resemble certain distributions (such as Gaussian or uniform). However, for $d=8$ table \ref{table:scalers} shows that this does not improve the quality too much.

The robust scaler transforms the points such that the 75th percentile are more packed to limit the influence of outliers, while the Gaussian and Uniform ones do exactly as their names say: they transform them to match the respective distributions.

\begin{table}[b!]
\centering
\label{table:scalers}
\begin{tabular}{| | c c c c c c | |}
	\hline
		Scaler & MAP & P@1 & P@3 & P@5 & P@10 \\
	\hline
		None & $.207$ & $.130$ & .115 & $.107$ & $.104$ \\
		Robust   & $.208$ & $.123$ & .112 & $.111$ & $.106$ \\
		Gaussian  & $.216$ & $.137$ & .123 & $.116$ & $.111$ \\
		Unfirom  & $.216$ & $.137$ & $.123$ & $.116$ & $.111$ \\
\hline
\end{tabular}
\caption{Results after applying different scalers on the dataset}
\end{table}

\section{Discussion}
\label{sec:results_discussion}

While the methodology might not be ideal due to the algorithm being unsupervised, some sort of comparison can be made by creating a parallel recommender with ground truth data. This project is compatible with a lot of models, but there aren't many pre-trained ones which perform better than those used. The results of the word2vec models are especially underwhelming.

The recommender works better than randomly selecting pictures from the dataset (P@1 precision $.137$ vs $.003$). While there are a lot of variances and error-prone situations, the model performs rather well, even though the object detection model mislabels input and some word2vec relations do not always make sense.

By increasing the number of dimensions of the hyperplane, the precision improves, but it is highly reliable on the keyword of each dimension, as seen in figures \ref{fig:dims} and \ref{fig:dims_map}. Scalers offer a way for re-placing points in plane to match a certain distribution based on the values on each dimension, but they have not shown to improve the overall results in a significant way
